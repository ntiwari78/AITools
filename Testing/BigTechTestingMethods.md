I’ll research how big tech companies currently handle software testing without dedicated SDET roles. This will include modern testing strategies, the role of developers in testing, the adoption of scalable automated testing tools, and the impact on software quality. I’ll provide insights into industry best practices and how different companies approach software testing today. I’ll get back to you with the findings soon.

# Testing Without Dedicated SDETs: How Big Tech Does It Today

## From SDET to “Quality is Everyone’s Job”  
Modern software teams at Big Tech firms have largely moved away from dedicated **Software Development Engineer in Test (SDET)** or standalone QA roles. Instead, quality assurance responsibilities are now embedded within engineering teams ([The Future of Ops. This article was originally published… | by Tyler Treat | Real Kinetic Blog](https://blog.realkinetic.com/the-future-of-ops-4e4842664445#:~:text=The%20future%20of%20Operations%20is,and%20Amazon%2C%20was%20the%20first)) ([The Future of Ops. This article was originally published… | by Tyler Treat | Real Kinetic Blog](https://blog.realkinetic.com/the-future-of-ops-4e4842664445#:~:text=step%20in%20this%20direction,test%20code%2C%20and%20tools%20code)). This shift began in the mid-2010s, when companies like Microsoft merged their developer (SDE) and tester (SDET) positions into a single **combined engineering** role ([The Future of Ops. This article was originally published… | by Tyler Treat | Real Kinetic Blog](https://blog.realkinetic.com/the-future-of-ops-4e4842664445#:~:text=step%20in%20this%20direction,test%20code%2C%20and%20tools%20code)). In practice, this means developers are expected to write and run tests for their own code, rather than “throwing code over the wall” to a separate QA team. Testing has become a core part of the developer’s job – a cultural change summed up by Google’s experience: *“The adoption of developer-driven automated testing has been one of the most transformational software engineering practices at Google”*, enabling larger systems to be built faster while keeping quality high ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=Conclusion)).

**Why eliminate dedicated testers?** The rapid pace of agile and continuous delivery demanded faster feedback and fewer handoffs. Having a separate test phase and test team often caused slow, ping-pong handoffs of bug fixes and an “us vs. them” dynamic between dev and QA ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,row%20in%20the%20same%20direction)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=%2A%20Ticket%20ping,I%E2%80%99d%20be%20willing%20to%20admit)). By making developers own quality, companies found they could tighten feedback loops and ship more frequently without sacrificing quality ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=long%20delay.%20,%E2%80%9D)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,%E2%80%9D)). For example, Microsoft’s Bing team shifted all programmatic testing to developers and kept only a small QA group for real-world user scenarios – this change **“improved the team’s ability to ship changes without harming overall software quality”** ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,%E2%80%9D)). In short, building quality in from the start (with developers writing tests) replaced the old model of testing quality in at the end.

## Modern Testing Strategies in Big Tech  
Today’s testing strategies emphasize **automation, early testing, and developer ownership**. Key practices include:

- **Developers write unit and integration tests:** It’s now standard at companies like Google and Microsoft that the engineer implementing a feature also writes the automated tests for it ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,%E2%80%9D)) ([Quote by James A. Whittaker: “Google has also benefitted from being at the in...”](https://www.goodreads.com/quotes/9337683-google-has-also-benefitted-from-being-at-the-inflection-point#:~:text=responsible%20for%20building%20components%20that,activity%20performed%20by%20the%20other)). Unit tests ensure new code works in isolation, while integration tests check components together. This shift-left approach catches bugs earlier and makes testing part of the development workflow, not a separate phase. Google calls this “developer-driven automated testing” and notes it has *“transformed [their] engineering culture to elevate testing into a cultural norm”* ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=Conclusion)).

- **Embedded “Quality/Eng Productivity” roles:** Instead of traditional QA teams, many organizations have **Engineering Productivity** or **Test Engineering** specialists who focus on building tools and infrastructure to help developers test effectively. At Google, **Test Engineers (TEs)** evolved from manual testers into automation experts who *“write scripts to automate testing, create tools so developers could test their own code, and [find] creative ways to break software”* ([
Google Testing Blog: From QA to Engineering Productivity
](https://testing.googleblog.com/2016/03/from-qa-to-engineering-productivity.html#:~:text=Manual%20operations%20were%20reduced%20to,weak%20spots%20and%20break%20software)). Likewise, Google’s **Software Engineers in Test (SETs)** (now often called Software Engineers in Productivity or Tools) develop internal frameworks and testing systems that scale across the company ([
Google Testing Blog: From QA to Engineering Productivity
](https://testing.googleblog.com/2016/03/from-qa-to-engineering-productivity.html#:~:text=SETs%20initially%20focused%20on%20building,in%20the%20industry%20and%20established)). These roles act as force multipliers, enabling each development team to test more efficiently (e.g. by providing reliable test frameworks, continuous integration systems, etc.) rather than doing all the testing themselves.

- **Comprehensive automation & CI/CD:** Big Tech relies on extensive automated test suites run in Continuous Integration pipelines. Every code change triggers a battery of tests in environments that can scale to thousands or even millions of tests per day. For instance, Google built and open-sourced many **scalable testing tools** – such as Selenium WebDriver for browser automation, Espresso for Android UI testing, and GoogleTest for C++ unit tests – to enable robust automation ([
Google Testing Blog: From QA to Engineering Productivity
](https://testing.googleblog.com/2016/03/from-qa-to-engineering-productivity.html#:~:text=SETs%20initially%20focused%20on%20building,in%20the%20industry%20and%20established)). Tests are categorized by size (small, medium, large) and run under strict constraints to keep them fast and deterministic ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=At%20Google%2C%20we%20classify%20every,2.4)) ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=We%20make%20this%20distinction%2C%20as,Let%E2%80%99s%20take%20a%20closer%20look)). This focus on speed lets developers get quick feedback from tests. At Facebook (Meta), engineering teams have built an “autonomous testing infrastructure” to run integration tests at scale, and they leverage static analysis tools (like their open-source **Infer** analyzer) to catch bugs early without manual effort. Companies also use cloud resources to run tests in parallel, and many practice **continuous delivery** – automatically deploying code to production once tests pass.

- **Shift-right and monitoring:** Another modern strategy is testing in production with careful monitoring – often used by firms practicing DevOps. Techniques like **canary releases** (gradually rolling out changes to a subset of users) and **feature flags** allow teams to expose new code slowly and watch real user metrics for anomalies. Netflix, for example, has no formal QA department; instead, developers write extensive automated tests and use sophisticated monitoring and **chaos engineering** tools (like Chaos Monkey) to ensure reliability in production. Issues that slip through are quickly detected and rolled back. In general, Big Tech companies complement pre-release testing with strong **observability** (logs, alerts, user feedback channels) to maintain quality without gating every change on manual QA.

- **Targeted manual testing where it matters:** Even without large QA teams, big companies haven’t abandoned manual testing entirely. They use it in areas where human judgment is irreplaceable or automation is hard. Google notes that assessing things like search result quality or UX nuances often *“involves human judgment”*, so they conduct internal studies with real users or exploratory testing for those cases ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=Automated%20testing%C2%A0%20is%20not%20suitable,calling%20systems)) ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=A%20more%C2%A0%20generalized%20term%20for,added%20to%20prevent%20future%20regressions)). Similarly, companies dogfood their own products: employees act as beta testers (e.g. Facebook’s internal builds or Google’s “fishfood” program) to get real-world usage before public release. This focused manual effort, combined with automated coverage of routine checks, ensures that human creativity is applied where it adds the most value (finding unexpected edge cases), rather than having humans repetitively verify known scenarios that machines can check ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=Using%20automated%20testing%20to%20cover,to%20tears%20in%20the%20process)).

## Developers as Testers: Roles and Responsibilities  
With dedicated tester roles shrinking, **developers now wear the tester hat** in most Big Tech engineering teams. Here’s how some major companies structure testing responsibilities today:

- **Microsoft:** Microsoft historically pioneered the SDET role and, until around 2014, teams often had a 2:1 ratio of developers to SDETs ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=A%202%3A1%20SDE%3ASDET%20ratio%20was,was%20composed%2C%20based%20on%20headcount)). SDETs wrote test code, built test frameworks, and performed manual testing of edge cases, working alongside developers ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=On%20our%20project%2C%20the%20SDET,owned%20all%20parts%20of%20testing)). This division, however, sometimes led to friction – developers might slack on writing unit tests or view testing as “outsourced” to SDETs ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=Having%20dedicated%20SDETs%20made%20it,test%20team%20write%20unit%20tests)), causing the classic dev/test handoff delays and silo mindset ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,row%20in%20the%20same%20direction)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,switching%20to%20a%20dev%20role)). In 2014, Microsoft **formally retired the SDET title**, merging SDETs and SDEs into a unified **“Software Engineer”** role responsible for both feature code and test code ([The Future of Ops. This article was originally published… | by Tyler Treat | Real Kinetic Blog](https://blog.realkinetic.com/the-future-of-ops-4e4842664445#:~:text=embedded%20within%20development%20teams,test%20code%2C%20and%20tools%20code)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=In%20July%202014%2C%20Microsoft%20announced,to%20SE%20%E2%80%93%20Software%20Engineer)). Many SDETs transitioned to developer roles, and everyone was now accountable for quality. Brian Harry of Microsoft observed this combined engineering approach increased developer accountability and dramatically sped up testing cycles: *“We eliminated the distinction between people who code and people who test… every person does some of everything and is accountable for the quality of what they produce.”* ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,%E2%80%9D)). After the change, teams rebalanced their test suites – **fewer slow end-to-end tests and many more unit tests** – making feedback loops much faster ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,%E2%80%9D)). Microsoft still has some central test-tool teams and uses manual testing for products like Windows or hardware, but the day-to-day testing on most software projects is owned by the engineers writing the code.

 ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa)) *Shift in Microsoft’s testing approach:* The Visual Studio Team Services group found that before combining dev and test roles, they relied heavily on slow end-to-end tests (top). Two years after merging SDETs into the dev team, the emphasis shifted to fast-running unit tests (bottom), greatly reducing reliance on brittle end-to-end checks ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,%E2%80%9D)). This illustrates the industry trend toward a **“test pyramid”** (many unit tests, fewer broad tests) once developers take direct ownership of testing.

- **Google:** In Google’s early years, they introduced two testing-focused roles: **Software Engineers in Test (SET)**, who were developers building test frameworks and automation, and **Test Engineers (TE)**, who focused on product risk areas and what should be tested ([
Google Testing Blog: From QA to Engineering Productivity
](https://testing.googleblog.com/2016/03/from-qa-to-engineering-productivity.html#:~:text=%2A%20Test%20Engineers%20%28TEs%29%20,packages%20required%20to%20implement%20automation)) ([
Google Testing Blog: From QA to Engineering Productivity
](https://testing.googleblog.com/2016/03/from-qa-to-engineering-productivity.html#:~:text=became%20go,weak%20spots%20and%20break%20software)). SETs and TEs worked to enable a small number of developers to test a large codebase through infrastructure and tooling. Over time, Google found that having engineers write more of their own tests was vital to keep up with scale. Their internal culture (spurred by the famed “Testing on the Toilet” initiative) encouraged every SWE (Software Engineer) to consider testing as part of development. Today, Google still has an **Engineering Productivity** organization (the evolved form of SET/TE roles) that builds advanced test tools and maintains the massive continuous testing systems. But Google’s product teams generally **do not have separate QA testers** – the feature developers themselves write unit tests and often larger integration tests, using the tools Eng Prod provides ([Quote by James A. Whittaker: “Google has also benefitted from being at the in...”](https://www.goodreads.com/quotes/9337683-google-has-also-benefitted-from-being-at-the-inflection-point#:~:text=responsible%20for%20building%20components%20that,activity%20performed%20by%20the%20other)) ([Quote by James A. Whittaker: “Google has also benefitted from being at the in...”](https://www.goodreads.com/quotes/9337683-google-has-also-benefitted-from-being-at-the-inflection-point#:~:text=code%20and%20unit%20test%20code,best%20attempt%20at%20achieving%20it)). A quote from *How Google Tests Software* explains the philosophy: *“Google SWEs are feature developers… They write feature code **and unit test code** for those features. Google SETs are test developers, responsible for assisting SWEs with the unit test portion… and writing larger test frameworks. Google TEs take the user’s perspective… creating automation for user scenarios and assessing overall test coverage.”* ([Quote by James A. Whittaker: “Google has also benefitted from being at the in...”](https://www.goodreads.com/quotes/9337683-google-has-also-benefitted-from-being-at-the-inflection-point#:~:text=responsible%20for%20building%20components%20that,best%20attempt%20at%20achieving%20it)). In practice, this means Google developers write tests at all levels, and specialized test engineers concentrate on scaling the testing effort (e.g. providing robust frameworks, continuous integration, and test result analytics). This approach has let Google eliminate traditional QA bottlenecks – by 2016 they noted manual testing was largely limited to exploratory checks on new features, while automation handles the rest ([
Google Testing Blog: From QA to Engineering Productivity
](https://testing.googleblog.com/2016/03/from-qa-to-engineering-productivity.html#:~:text=Manual%20operations%20were%20reduced%20to,weak%20spots%20and%20break%20software)). Even as Google grew 100x in size, making testing a shared responsibility kept quality high without needing a big QA headcount ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=Conclusion)).

- **Amazon:** Amazon is an interesting case – it retains more dedicated QA and SDET roles than some peers ([Quality Assurance Across the Tech Industry](https://newsletter.pragmaticengineer.com/p/qa-across-tech#:~:text=Meta%2C%20Apple%2C%20Amazon%2C%20Uber%20and,QA)). In Amazon’s “two-pizza teams” (small, autonomous product teams), it’s not uncommon to find **QA Engineers** or **SDETs** embedded to focus on testing and automation for that service. Amazon has a reputation for a strong “you build it, you run it” DevOps culture, which often includes testing: developers are expected to write unit tests and own the reliability of their code in production. But Amazon also values dedicated test roles in certain areas, especially where complex integration or hardware is involved. According to industry analysis, **Apple and Amazon are two big tech companies that still place a high focus on dedicated QA engineers or SDETs** compared to others ([Quality Assurance Across the Tech Industry](https://newsletter.pragmaticengineer.com/p/qa-across-tech#:~:text=Meta%2C%20Apple%2C%20Amazon%2C%20Uber%20and,QA)). For example, Amazon’s devices (like Alexa, Echo, and Kindle) and consumer-facing features have SDETs who build automated test suites and tools, working alongside developers. Job postings for Amazon SDETs describe responsibilities like *“driving test automation and tooling and working closely with the central QA and tech teams”* ([Software Development Engineer in Test (SDET) - II, Alexa Global ...](https://www.amazon.jobs/en/jobs/2792409/software-development-engineer-in-test-sdet-ii-alexa-global-quality#:~:text=Software%20Development%20Engineer%20in%20Test,central%20QA%20and%20tech%20teams)). This suggests Amazon uses a hybrid model: **developers write a lot of tests, but QA specialists are there to develop complex test automation, ensure end-to-end scenarios are covered, and perform manual testing where needed**. The net effect is similar to others – heavy automation and developer responsibility – but Amazon hasn’t entirely done away with the SDET title internally. Still, the trend is toward more automation; Amazon even leverages AI-assisted testing (e.g. its internal tool “Amazon CodeGuru (formerly Q)" can auto-generate some unit tests ([Testing your applications with Amazon Q Developer - AWS](https://aws.amazon.com/blogs/devops/testing-your-applications-with-amazon-q-developer/#:~:text=In%20this%20blog%20post%2C%20we,automating%20test%20scenarios%20and))) to support developers. In summary, **Amazon’s engineers share quality ownership with QA experts**, and the emphasis is on scalable testing (their services often require testing distributed systems at scale) combined with vigilant production monitoring (Amazon monitors metrics and customer feedback closely to detect issues quickly).

- **Meta (Facebook):** Facebook took an extreme stance early on – **no dedicated QA department at all** for its software products. This “no tester” approach was noted around 2014, when Facebook’s engineering leaders announced that they rely on developers and automated systems to assure quality ([Facebook’s No-Testing-Department Approach: An Interview with Simon Stewart | StickyMinds](https://www.stickyminds.com/interview/facebook-s-no-testing-department-approach-interview-simon-stewart#:~:text=In%20this%20TechWell%20interview%2C%20Facebook%27s,different%20mobile%20builds%20per%20year)). Instead of traditional QA, Facebook invests in tooling and practices to enable rapid deployment with confidence. Every engineer at Meta is expected to test their code (with peer code reviews ensuring tests are included), and the company uses a sophisticated continuous deployment pipeline to ship code to production twice a day. To manage quality, Facebook employs strategies like **extensive static analysis**, property-based tests, and an internal stress testing framework. They also run one of the world’s largest beta-testing communities (with employees and volunteers using “dogfood” builds of Facebook apps) to catch issues. As Simon Stewart (a Facebook engineering manager) explained, this rapid release model means **“there’s no space to keep QA at arm’s length until the end… quality isn’t something you can add as an afterthought”** ([Facebook’s No-Testing-Department Approach: An Interview with Simon Stewart | StickyMinds](https://www.stickyminds.com/interview/facebook-s-no-testing-department-approach-interview-simon-stewart#:~:text=Josiah%20Renaudin%3A%20You%20mention%20in,of%20skills%20to%20remain%20marketable)) ([Facebook’s No-Testing-Department Approach: An Interview with Simon Stewart | StickyMinds](https://www.stickyminds.com/interview/facebook-s-no-testing-department-approach-interview-simon-stewart#:~:text=In%20order%20to%20enable%20fast,They%20may%20find%20that)). Testers at Facebook needed to become more technical (many QA-minded folks learned to code and joined teams as software engineers in productivity or tooling roles). By not having a separate QA gate, Facebook can maintain a high development velocity – but it compensates with strong discipline in automated testing and the ability to quickly fix or roll back problems in production. This approach has since influenced other Silicon Valley companies to empower engineers to fully own quality.

- **Apple:** In contrast to the others, Apple has famously high standards for product quality and still employs a significant number of dedicated **QA/Test Engineers** (especially for its operating systems, apps, and hardware devices). Apple organizes specialized QA teams for different product lines – e.g. an iOS QA team, a hardware test team for devices, etc. These QA engineers perform extensive manual and automated testing on pre-release versions of products, as Apple’s longer release cycles (annual iOS releases, for instance) allow for thorough in-house testing. Industry reports note that Apple is one of the few Big Tech companies that **continues to invest heavily in dedicated QA roles** ([Quality Assurance Across the Tech Industry](https://newsletter.pragmaticengineer.com/p/qa-across-tech#:~:text=Meta%2C%20Apple%2C%20Amazon%2C%20Uber%20and,QA)). That said, Apple’s developers are not off the hook on testing: they still write unit tests and have to meet quality gates in the build process. Apple combines developer-written tests with rigorous QA verification (including large regression test suites, UI automation for their apps, and intensive real-world scenario testing). This dual approach is partly why Apple’s software is known for polish – code is tested by its authors and then again by independent QA teams who approach it from a user’s perspective. Additionally, because Apple integrates software with custom hardware, specialized test engineers are needed to validate things like performance, battery usage, and device integration that regular software developers might not fully cover. In summary, Apple’s structure is a bit more traditional: **developers + separate QA**, working hand-in-hand to maintain a standard of near-zero bugs in consumer releases.

- **Other Companies (Uber, Netflix, etc.):** Many other tech companies follow the “no dedicated QA” model. Uber and Netflix, for example, generally **do not have separate test teams for their software services** ([Quality Assurance Across the Tech Industry](https://newsletter.pragmaticengineer.com/p/qa-across-tech#:~:text=Meta%2C%20Apple%2C%20Amazon%2C%20Uber%20and,QA)). Developers at these companies build and test features, relying on automation and progressive delivery. Netflix’s engineering culture entrusts each dev team with end-to-end ownership (including testing and monitoring in production). However, niche areas like Netflix’s device partnerships (e.g. Netflix app on smart TVs) do have **Test Automation Engineers** to ensure the app works on countless platforms – these roles focus on writing automated tests for UI and video playback across devices. Similarly, in the gaming industry or other specialized fields, companies might still employ QA testers for exploratory testing and gameplay experience, but even there we see a push for developers to create robust automation for things like game engine testing. Overall, the **industry consensus** has shifted: for most software products, a **developer-centric testing approach** is preferred, with perhaps a few testing experts per team or per organization to support and advise.

## Scalable Automated Testing Tools and Techniques  
To make this developer-driven model work, Big Tech companies heavily leverage **scalable testing tools and infrastructure**:

- **In-House Frameworks:** Google’s SETs built a wide array of internal tools to automate every aspect of testing, many of which were later shared with the world ([
Google Testing Blog: From QA to Engineering Productivity
](https://testing.googleblog.com/2016/03/from-qa-to-engineering-productivity.html#:~:text=SETs%20initially%20focused%20on%20building,in%20the%20industry%20and%20established)). These include frameworks like **Selenium WebDriver** (co-created by Google for browser automation), **Protractor** (for AngularJS apps), **Espresso** (Android UI testing), **EarlGrey** (iOS UI testing, open-sourced by Google), and **GoogleTest** (C++ testing framework). Such tools allow tests to be written once and run reliably at scale across thousands of machines. Microsoft and Amazon likewise use and contribute to frameworks: e.g. Microsoft has the **Visual Studio Unit Testing Framework** and supports open-source tools like Selenium for web testing in Azure DevOps pipelines. Amazon uses a mix of open-source (JUnit, TestNG, Selenium) and internal tools for things like service API testing and UI automation on their e-commerce site. 

- **Continuous Integration Systems:** A crucial piece is the CI system that can distribute and execute tests rapidly. Google’s internal CI can run **massive numbers of tests in parallel** across its data centers, with intelligent scheduling to keep overall build times short even as test count grows. Facebook built a system to run all tests on each code diff within minutes, so developers get near-immediate feedback. At these scales, companies invest in eliminating flaky tests (tests that fail randomly) and optimizing test runtime. They classify tests by size and automatically enforce timeouts or resource limits to keep suites stable ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=At%20Google%2C%20we%20classify%20every,2.4)) ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=We%20make%20this%20distinction%2C%20as,Let%E2%80%99s%20take%20a%20closer%20look)). Many have a dashboard to track test results, coverage, and flakiness, helping teams maintain healthy tests. This automation at scale is the backbone that allows **fast, safe releases** – even without a manual tester running through checklists, the automated suite acts as a safety net, and it’s run on every code change.

- **Test Coverage and Quality Metrics:** Engineering teams use metrics to ensure the testing is thorough. **Code coverage** (the percentage of code exercised by tests) is one common metric – Google and Microsoft both have guidelines or requirements for coverage, especially on critical code. Some teams employ stricter techniques like **mutation testing** (verifying tests catch injected faults) to gauge test effectiveness. Additionally, tracking the rate of production bugs or incidents is used to evaluate quality. If bugs escape to users, teams perform post-mortems to identify how testing could improve (e.g. writing a new regression test to catch that class of bug in the future ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=A%20more%C2%A0%20generalized%20term%20for,added%20to%20prevent%20future%20regressions)) ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=Using%20automated%20testing%20to%20cover,to%20tears%20in%20the%20process))). In the absence of a QA department acting as gatekeeper, these metrics and feedback loops keep quality in check. Companies also encourage a culture of quality via peer review: it’s common that when a developer opens a code review, they must also include **tests** and their peers will critique both the code and the tests before approval.

- **Specialized Testing (Performance, Security, etc.):** Scalable tools aren’t just for functional tests. Big Tech also uses automation for performance testing (e.g. frameworks to simulate high load on services and measure latency), for security testing (automated vulnerability scanners and fuzzing tools running continuously), and for other concerns like localization (scripts to verify software works in different languages). Many of these are integrated into the pipeline. Google, for instance, runs a continuous **fuzzing service (OSS-Fuzz)** on critical components to catch memory errors or security issues automatically. Microsoft and Amazon integrate static analysis and security tests as part of the build to catch issues early. All of this means quality is checked on many fronts without waiting for a human tester at the end.

- **Emerging Tools (AI in Testing):** An emerging trend is using AI to assist in testing at scale. Some companies have experimented with AI-driven test generation – for example, Amazon’s **“Amazon CodeWhisperer / Q”** uses AI to suggest or generate unit tests for code ([Testing your applications with Amazon Q Developer - AWS](https://aws.amazon.com/blogs/devops/testing-your-applications-with-amazon-q-developer/#:~:text=In%20this%20blog%20post%2C%20we,automating%20test%20scenarios%20and)). This can help developers write more tests with less effort. Likewise, tools that analyze code changes and select the most relevant subset of tests to run (to speed up feedback) are in use at Facebook and others (often leveraging machine learning on historical test data). While still maturing, these approaches show how automation is expanding even further to support developer testing.

## Impact on Software Quality and Team Dynamics  
**Has removing dedicated testers hurt or improved software quality?** The answer can depend on execution, but many tech giants report positive outcomes. When done right, shifting testing to developers has several benefits:

- **Faster Release Cycles:** With no waiting for a separate QA phase, teams iterate much more quickly. Microsoft found that after combining dev and test roles, teams that used to ship every few weeks could ship **daily** ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=Early%202014%2C%20I%20joined%20a,every%20day%2C%20not%20every%20month)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=followed%20the%20usual%20Scrum%20processes,This%20was%20a%20long%20delay)). At Facebook, releasing twice a day to millions of users is only possible because engineers get immediate test feedback and can rely on automated checks rather than lengthy manual regressions. This speed is a competitive advantage in deploying new features and fixes.

- **Higher Accountability and Quality Ownership:** Developers become more careful when they know they are responsible for catching their own bugs. In the old model, a dev might consider a feature “done” when coding finished and assume QA will find the rest. Now, with no safety net of a dedicated tester, engineers build more robustly and consider edge cases upfront ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=Those%20of%20us%20who%E2%80%99d%20previously,I%20was%20in%20this%20camp)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=The%20best%20part%20of%20this,discovered%2C%20before%20shipping%20to%20production)). Microsoft’s Brian Harry noted that under the separated model, developers had “little motivation to make their code testable” and would introduce bugs that took days to discover and fix, whereas the combined approach made quality a shared responsibility from the start ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=advantages%20of%20this%20model%20%E2%80%93,run%2C%20many%20more%20hours%20to)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,%E2%80%9D)). In essence, quality issues are no longer “someone else’s problem.”

- **Better Test Coverage and Infrastructure:** Interestingly, empowering developers to own testing has led to **more investment in better testing tools**. Google’s culture of “testing is everyone’s job” yielded an explosion of internal tools and best practices that improved test efficiency across the board ([
Google Testing Blog: From QA to Engineering Productivity
](https://testing.googleblog.com/2016/03/from-qa-to-engineering-productivity.html#:~:text=SETs%20initially%20focused%20on%20building,in%20the%20industry%20and%20established)). At Microsoft, after the SDET merger, the Visual Studio team literally threw away 8 years worth of slow tests and replaced them with a new suite designed by the combined engineering team ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=motivation%20to%20make%20their%20code,)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=the%20most%20part%2C%20we%20eliminated,%E2%80%9D)). The result was a shift to more unit tests (which are faster and easier to maintain) and a reduction in flaky, brittle tests. The overall quality did not drop – in fact, many teams saw **quality improvements** because issues were found and fixed earlier in development. Google’s experience is that developer-driven testing enabled them to build larger systems *“faster than we ever thought possible”* while keeping a strong commitment to quality ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=Conclusion)).

- **Challenges and Mitigations:** That said, this model isn’t without challenges. One risk is that developers might not have the **testing mindset or skills** that dedicated QA engineers did, potentially leading to blind spots. Some companies saw a dip in test rigor during transitions. For instance, when job platform Indeed abruptly eliminated all QA roles in 2023, an internal engineer observed *“the overall quality of tests has nosedived”* ([The QA role is changing, but QA will never die](https://qase.io/blog/the-qa-role-is-changing/#:~:text=In%20March%202023%2C%20job%20listings,%E2%80%9D)) – developers suddenly had to maintain tests they didn’t write, and some tests were abandoned. The lesson is that simply cutting QA doesn’t guarantee success; organizations must **train and support developers** in good testing practices. Many have addressed this by hiring or retraining QA staff into **“Quality Coaches”** or **“Testing Advocates”** who work within teams to mentor developers on writing effective tests, designing for testability, and thinking like a user. There’s also a potential cultural issue: not every engineer initially embraces testing. It takes strong leadership and examples (as Google’s Testing Grouplet demonstrated by seeding culture rather than imposing mandates ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=Why%20didn%E2%80%99t%20we%20start%20by,mandating%20the%20writing%20of%20tests))) to get full buy-in. Companies often establish internal best practices, do brownbag sessions on testing, and make testing an explicit part of performance evaluations to reinforce its importance.

- **When are dedicated QA still needed?:** Big Tech companies recognize that in certain scenarios, having testing specialists **adds critical value**. As mentioned, Apple still finds dedicated QA indispensable for its polished user experiences. In hardware or tightly integrated systems, the cost of failure is high, so extra testing eyes are worth it. Also, for new startups or teams without mature processes, having a QA engineer can initially boost quality until the developer testing culture matures. The prevailing philosophy is to **use human testers strategically** – for exploratory testing, usability feedback, or building out test scaffolds – rather than for rote regression checking. In many organizations, former QA folks have transitioned to roles like **Product Managers or Analysts**, where they contribute to quality by clarifying requirements and user expectations up front, or to **Site Reliability Engineering (SRE)** roles, focusing on quality of service in production. The overall impact on software quality in the industry has been largely positive: software is getting to users faster, and automation catches the majority of issues. End-user bug rates for services by Google, Microsoft (Azure, Office 365), and others remain low even as they ship updates daily – a sign that the new testing approaches can work as well as (or better than) the old ones.

## Industry Best Practices and Evolving Methodologies  
Over the past decade, the software testing methodology across the industry has **undergone a notable shift**. Some best practices and trends that have emerged include:

- **Test Early, Test Often (Shift Left):** It’s now best practice to incorporate testing from the earliest stages of development. Writing tests in parallel with code (or even before code, as in TDD – Test-Driven Development) is encouraged. This catches defects when they are cheapest to fix and ingrains quality thinking in design. Many teams automate test execution on every commit or even every code change via pre-commit hooks and CI triggers.

- **Test Pyramid & Right-Sizing Tests:** Teams strive for a **balanced test portfolio** – a large base of unit tests, a layer of integration tests, and a small tip of end-to-end tests. This “pyramid” minimizes slow, flaky tests and maximizes fast, reliable ones. Google explicitly classifies tests by size and pushes for as many “small” (quick) tests as possible for any given functionality ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=At%20Google%2C%20we%20classify%20every,2.4)) ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=The%20adoption%20of%20developer,than%20it%20has%20ever%20been)). End-to-end UI tests are used sparingly for critical user journeys. By following this approach, companies reduce the reliance on manual regression testing because the automated pyramid gives broad coverage.

- **Continuous Integration & Delivery:** It’s hard to overstate the impact of CI/CD on testing practices. The norm now is to integrate code frequently and run tests on an ongoing basis. A failed test in CI blocks the code from merging or releasing, ensuring that broken code doesn’t propagate. Continuous Delivery means every code change that passes tests can immediately go live. This forces teams to keep the test suite green and comprehensive at all times, effectively baking quality checks into the development pipeline.

- **“Quality Assistance” over Quality Assurance:** A modern mantra (sometimes called **“modern testing” principles) is that the role of a quality specialist is to **assist and enable the team** to own quality, not to be solely responsible for it. This is summed up by the idea that *quality is a team responsibility.* Companies like Atlassian coined the term “Quality Assistance” for their approach: a small QA group acts as consultants, providing tools, setting up frameworks, and occasionally doing exploratory testing, but developers do the bulk of testing work. This aligns with what we see at Google’s Eng Prod teams or Microsoft’s tools teams – the focus is on empowering engineers with the right testing capabilities. 

- **Infrastructure as Code & Test Environments:** Provisioning test environments on demand and using containerization or virtualization is a best practice that has grown. Instead of a separate QA environment that testers manually configure, today’s automated pipelines spin up test environments (containers, cloud test labs, etc.) as part of the run. This allows scalable and consistent test execution. For example, some companies use **Docker containers to simulate microservice dependencies** or employ cloud services like AWS Device Farm to run mobile app tests on many real devices in parallel.

- **Regression Testing and Monitoring in Production:** The philosophy around bugs has also evolved. Instead of trying to test absolutely everything pre-release (which can never guarantee zero bugs), the practice is to do thorough automated regression testing on core functionality, then use production monitoring to catch any unexpected issues quickly. Feature flags and rollback mechanisms have become a quality safety net – if a new deployment has a problem, teams can disable that feature or roll back the update within minutes. This reduces the fear of deploying and allows testing to be an iterative, ongoing activity even after release. In effect, some testing is deferred to production, but under controlled and observable conditions. This practice is sometimes called **“shift right”** – complementing shift-left – and is a hallmark of mature DevOps organizations.

- **Collaboration and Knowledge Sharing:** Teams now regularly include testing considerations in design discussions and code reviews. It’s a best practice to ask in design docs: *“How will we test this?”* and to plan for test hooks or simulators if needed. Peer code reviews often have a checklist item: “Are there sufficient tests for this change?” Many companies also share internal case studies of quality issues and fixes (e.g. a postmortem of an outage might result in new tests and be shared as a lesson learned). This continuous learning helps teams improve their testing effectiveness over time.

- **Tooling and Automation First Mindset:** Whenever a repetitive testing task is identified, engineers seek to automate it. This mindset has led to frameworks for everything – UI snapshots, API contract testing, dependency analysis, etc. A best practice is to integrate such tools early in a project so they scale with the codebase. For instance, setting up linters, static analyzers, and unit test harnesses on day one of a new service. It’s much easier to maintain high quality if the plumbing for tests and checks is laid down from the start.

In the last decade, the software industry has essentially **redefined the QA role**. We’ve moved from an era where testers were a separate safeguard, to an era where quality is built into the fabric of the development process through automation and shared responsibility. Most Big Tech companies no longer hire armies of manual testers; instead, they hire engineers who can both develop and test, or specialists who create the tools to make testing efficient. The result is software that can be released rapidly and reliably. While the exact approaches vary – some like Apple still lean on dedicated QA for certain products, while others like Facebook have none at all – the common thread is **heavy use of scalable automated testing and a culture of quality ownership**. As this trend continues, we can expect further innovation (such as AI-driven testing, even smarter automation, and ever-faster deployment pipelines) to support developers in delivering bug-free software without the need for a traditional testing department. In short, the modern best practice in big tech is: *if you wrote the code, you are also responsible for testing it* – and thanks to advanced tools and methodologies, developers are meeting that challenge and maintaining high software quality. ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,%E2%80%9D)) ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=Conclusion))

**Sources:**

1. Treat, T. (2018). *The Future of Ops – Real Kinetic Blog* – Discusses how QA roles shifted to tools-focused and how Microsoft merged SDET into SDE in 2014 ([The Future of Ops. This article was originally published… | by Tyler Treat | Real Kinetic Blog](https://blog.realkinetic.com/the-future-of-ops-4e4842664445#:~:text=The%20future%20of%20Operations%20is,and%20Amazon%2C%20was%20the%20first)) ([The Future of Ops. This article was originally published… | by Tyler Treat | Real Kinetic Blog](https://blog.realkinetic.com/the-future-of-ops-4e4842664445#:~:text=step%20in%20this%20direction,test%20code%2C%20and%20tools%20code)).  
2. Orosz, G. (2023). *How Big Tech does QA – The Pragmatic Engineer* – Details on Microsoft’s SDET history and 2014 combined engineering change ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=Web%20teams%20across%20Microsoft%20started,the%20way%20they%20always%20had)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,%E2%80%9D)), including an Ars Technica quote on Bing’s success moving tests to developers ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,%E2%80%9D)) and Brian Harry’s reflection on merging dev/test at Microsoft ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,%E2%80%9D)).  
3. Google Testing Blog (2016). *From QA to Engineering Productivity* – Describes evolution of Test Engineers (TEs) and SETs at Google into roles that build automation tools and enable developers to test their own code ([
Google Testing Blog: From QA to Engineering Productivity
](https://testing.googleblog.com/2016/03/from-qa-to-engineering-productivity.html#:~:text=Manual%20operations%20were%20reduced%20to,weak%20spots%20and%20break%20software)) ([
Google Testing Blog: From QA to Engineering Productivity
](https://testing.googleblog.com/2016/03/from-qa-to-engineering-productivity.html#:~:text=SETs%20initially%20focused%20on%20building,in%20the%20industry%20and%20established)).  
4. Whittaker, J. (2012). *How Google Tests Software* – Explains Google’s use of SWEs, SETs, and TEs and their respective testing responsibilities ([Quote by James A. Whittaker: “Google has also benefitted from being at the in...”](https://www.goodreads.com/quotes/9337683-google-has-also-benefitted-from-being-at-the-inflection-point#:~:text=responsible%20for%20building%20components%20that,activity%20performed%20by%20the%20other)) ([Quote by James A. Whittaker: “Google has also benefitted from being at the in...”](https://www.goodreads.com/quotes/9337683-google-has-also-benefitted-from-being-at-the-inflection-point#:~:text=code%20and%20unit%20test%20code,best%20attempt%20at%20achieving%20it)).  
5. Renaudin, J. (2014). *Interview with Simon Stewart – Facebook’s No-Testing-Department Approach* (StickyMinds) – Confirms that Facebook has no separate QA department and developers handle testing, as part of a trend toward faster releases ([Facebook’s No-Testing-Department Approach: An Interview with Simon Stewart | StickyMinds](https://www.stickyminds.com/interview/facebook-s-no-testing-department-approach-interview-simon-stewart#:~:text=In%20this%20TechWell%20interview%2C%20Facebook%27s,different%20mobile%20builds%20per%20year)) ([Facebook’s No-Testing-Department Approach: An Interview with Simon Stewart | StickyMinds](https://www.stickyminds.com/interview/facebook-s-no-testing-department-approach-interview-simon-stewart#:~:text=Josiah%20Renaudin%3A%20You%20mention%20in,of%20skills%20to%20remain%20marketable)).  
6. Orosz, G. (2023). *QA Across the Tech Industry – The Pragmatic Engineer* – Notes that Apple and Amazon still maintain dedicated QA/SDET roles and focus, whereas most other tech giants do not ([Quality Assurance Across the Tech Industry](https://newsletter.pragmaticengineer.com/p/qa-across-tech#:~:text=Meta%2C%20Apple%2C%20Amazon%2C%20Uber%20and,QA)).  
7. Moore, N. (2024). *“The QA role is changing, but will never die” – Qase.io* – Discusses Indeed’s 2023 removal of QA roles and the reported drop in test quality afterward ([The QA role is changing, but QA will never die](https://qase.io/blog/the-qa-role-is-changing/#:~:text=In%20March%202023%2C%20job%20listings,%E2%80%9D)), illustrating the importance of maintaining strong testing practices when shifting QA to developers.  
8. “Software Engineering at Google” (2020) – Google’s internal engineering book (Abseil.io) – Highlights the cultural transformation achieved by developer-driven testing at Google ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=Conclusion)) and emphasizes the need for manual exploratory testing in certain areas despite heavy automation ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=Automated%20testing%C2%A0%20is%20not%20suitable,calling%20systems)) ([Software Engineering at Google](https://abseil.io/resources/swe-book/html/ch11.html#:~:text=A%20more%C2%A0%20generalized%20term%20for,added%20to%20prevent%20future%20regressions)).  
9. Microsoft DevBlogs (2017) – Brian Harry’s account of Visual Studio Team Services’ overhaul of testing post-SDET-merger ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,)) ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,%E2%80%9D)), showing a switch from end-to-end heavy testing to a unit-test-focused strategy.  
10. Ars Technica (2014) – *“Microsoft drops SDET role”* (via Pragmatic Engineer excerpt) – Describes how Bing moved automated testing to developers and kept QA for exploratory testing, improving quality and speed ([How Big Tech does Quality Assurance (QA) - by Gergely Orosz](https://newsletter.pragmaticengineer.com/p/how-big-tech-does-qa#:~:text=,%E2%80%9D)).
